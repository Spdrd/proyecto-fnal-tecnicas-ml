{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8e4c76",
   "metadata": {},
   "source": [
    "\"Exploratorio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531a44d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edad</th>\n",
       "      <th>Género</th>\n",
       "      <th>Estado civil</th>\n",
       "      <th>Altura</th>\n",
       "      <th>Peso</th>\n",
       "      <th>Índice de masa corporal</th>\n",
       "      <th>¿Fuma actualmente?</th>\n",
       "      <th>¿Fumó en el pasado?</th>\n",
       "      <th>¿Consume alcohol frecuentemente?</th>\n",
       "      <th>Nivel de actividad física</th>\n",
       "      <th>...</th>\n",
       "      <th>¿Sufre de problemas de visión?</th>\n",
       "      <th>¿Tiene problemas de audición?</th>\n",
       "      <th>¿Ha sufrido de fracturas óseas en el pasado?</th>\n",
       "      <th>Nivel de satisfacción con la vida</th>\n",
       "      <th>Enfermedad cardiovascular</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Asma</th>\n",
       "      <th>Cáncer</th>\n",
       "      <th>Obesidad</th>\n",
       "      <th>Depresión/Ansiedad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.596326</td>\n",
       "      <td>Otro</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>153.681426</td>\n",
       "      <td>76.920289</td>\n",
       "      <td>29.612895</td>\n",
       "      <td>No</td>\n",
       "      <td>Sí</td>\n",
       "      <td>No</td>\n",
       "      <td>Moderado</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Medio</td>\n",
       "      <td>0.674302</td>\n",
       "      <td>-0.171059</td>\n",
       "      <td>1.142946</td>\n",
       "      <td>0.293202</td>\n",
       "      <td>-0.130552</td>\n",
       "      <td>0.228336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.795297</td>\n",
       "      <td>Otro</td>\n",
       "      <td>Casado</td>\n",
       "      <td>155.882307</td>\n",
       "      <td>66.743641</td>\n",
       "      <td>9.902543</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Moderado</td>\n",
       "      <td>...</td>\n",
       "      <td>Sí</td>\n",
       "      <td>No</td>\n",
       "      <td>Sí</td>\n",
       "      <td>Bajo</td>\n",
       "      <td>-0.014915</td>\n",
       "      <td>0.101641</td>\n",
       "      <td>-0.059422</td>\n",
       "      <td>1.047785</td>\n",
       "      <td>0.216788</td>\n",
       "      <td>1.211533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.603394</td>\n",
       "      <td>Otro</td>\n",
       "      <td>Casado</td>\n",
       "      <td>176.481841</td>\n",
       "      <td>124.818134</td>\n",
       "      <td>27.248719</td>\n",
       "      <td>Sí</td>\n",
       "      <td>No</td>\n",
       "      <td>Sí</td>\n",
       "      <td>Moderado</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Medio</td>\n",
       "      <td>0.981927</td>\n",
       "      <td>0.054446</td>\n",
       "      <td>0.918964</td>\n",
       "      <td>0.138127</td>\n",
       "      <td>-0.030003</td>\n",
       "      <td>-0.205018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.154276</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>Viudo</td>\n",
       "      <td>158.681358</td>\n",
       "      <td>114.807668</td>\n",
       "      <td>27.634473</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Moderado</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Sí</td>\n",
       "      <td>No</td>\n",
       "      <td>Bajo</td>\n",
       "      <td>1.147131</td>\n",
       "      <td>0.256350</td>\n",
       "      <td>-0.159599</td>\n",
       "      <td>-0.260462</td>\n",
       "      <td>1.363624</td>\n",
       "      <td>0.291855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.176676</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Casado</td>\n",
       "      <td>184.451263</td>\n",
       "      <td>60.217207</td>\n",
       "      <td>24.094841</td>\n",
       "      <td>No</td>\n",
       "      <td>Sí</td>\n",
       "      <td>No</td>\n",
       "      <td>Sedentario</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Medio</td>\n",
       "      <td>1.067995</td>\n",
       "      <td>0.225792</td>\n",
       "      <td>0.165198</td>\n",
       "      <td>0.015367</td>\n",
       "      <td>0.960565</td>\n",
       "      <td>1.427700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Edad     Género Estado civil      Altura        Peso  \\\n",
       "0  76.596326       Otro      Soltero  153.681426   76.920289   \n",
       "1  79.795297       Otro       Casado  155.882307   66.743641   \n",
       "2  90.603394       Otro       Casado  176.481841  124.818134   \n",
       "3  22.154276   Femenino        Viudo  158.681358  114.807668   \n",
       "4  46.176676  Masculino       Casado  184.451263   60.217207   \n",
       "\n",
       "   Índice de masa corporal ¿Fuma actualmente? ¿Fumó en el pasado?  \\\n",
       "0                29.612895                 No                  Sí   \n",
       "1                 9.902543                 No                  No   \n",
       "2                27.248719                 Sí                  No   \n",
       "3                27.634473                 No                  No   \n",
       "4                24.094841                 No                  Sí   \n",
       "\n",
       "  ¿Consume alcohol frecuentemente? Nivel de actividad física  ...  \\\n",
       "0                               No                  Moderado  ...   \n",
       "1                               No                  Moderado  ...   \n",
       "2                               Sí                  Moderado  ...   \n",
       "3                               No                  Moderado  ...   \n",
       "4                               No                Sedentario  ...   \n",
       "\n",
       "  ¿Sufre de problemas de visión? ¿Tiene problemas de audición?  \\\n",
       "0                             No                            No   \n",
       "1                             Sí                            No   \n",
       "2                             No                            No   \n",
       "3                             No                            Sí   \n",
       "4                             No                            No   \n",
       "\n",
       "  ¿Ha sufrido de fracturas óseas en el pasado?  \\\n",
       "0                                           No   \n",
       "1                                           Sí   \n",
       "2                                           No   \n",
       "3                                           No   \n",
       "4                                           No   \n",
       "\n",
       "  Nivel de satisfacción con la vida Enfermedad cardiovascular  Diabetes  \\\n",
       "0                             Medio                  0.674302 -0.171059   \n",
       "1                              Bajo                 -0.014915  0.101641   \n",
       "2                             Medio                  0.981927  0.054446   \n",
       "3                              Bajo                  1.147131  0.256350   \n",
       "4                             Medio                  1.067995  0.225792   \n",
       "\n",
       "       Asma    Cáncer  Obesidad Depresión/Ansiedad  \n",
       "0  1.142946  0.293202 -0.130552           0.228336  \n",
       "1 -0.059422  1.047785  0.216788           1.211533  \n",
       "2  0.918964  0.138127 -0.030003          -0.205018  \n",
       "3 -0.159599 -0.260462  1.363624           0.291855  \n",
       "4  0.165198  0.015367  0.960565           1.427700  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "file_path = 'health_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a244e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed29bb",
   "metadata": {},
   "source": [
    "# Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b00f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edad</th>\n",
       "      <th>Género</th>\n",
       "      <th>Estado civil</th>\n",
       "      <th>Altura</th>\n",
       "      <th>Peso</th>\n",
       "      <th>Índice de masa corporal</th>\n",
       "      <th>¿Fuma actualmente?</th>\n",
       "      <th>¿Fumó en el pasado?</th>\n",
       "      <th>¿Consume alcohol frecuentemente?</th>\n",
       "      <th>Nivel de actividad física</th>\n",
       "      <th>...</th>\n",
       "      <th>Asma_real</th>\n",
       "      <th>Cáncer_real</th>\n",
       "      <th>Obesidad_real</th>\n",
       "      <th>Depresión/Ansiedad_real</th>\n",
       "      <th>Enfermedad cardiovascular_bin</th>\n",
       "      <th>Diabetes_bin</th>\n",
       "      <th>Asma_bin</th>\n",
       "      <th>Cáncer_bin</th>\n",
       "      <th>Obesidad_bin</th>\n",
       "      <th>Depresión/Ansiedad_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.596326</td>\n",
       "      <td>Otro</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>153.681426</td>\n",
       "      <td>76.920289</td>\n",
       "      <td>29.612895</td>\n",
       "      <td>No</td>\n",
       "      <td>Sí</td>\n",
       "      <td>No</td>\n",
       "      <td>Moderado</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607766</td>\n",
       "      <td>0.206125</td>\n",
       "      <td>0.185332</td>\n",
       "      <td>0.526435</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.795297</td>\n",
       "      <td>Otro</td>\n",
       "      <td>Casado</td>\n",
       "      <td>155.882307</td>\n",
       "      <td>66.743641</td>\n",
       "      <td>9.902543</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Moderado</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127169</td>\n",
       "      <td>0.464862</td>\n",
       "      <td>0.353357</td>\n",
       "      <td>1.062676</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.603394</td>\n",
       "      <td>Otro</td>\n",
       "      <td>Casado</td>\n",
       "      <td>176.481841</td>\n",
       "      <td>124.818134</td>\n",
       "      <td>27.248719</td>\n",
       "      <td>Sí</td>\n",
       "      <td>No</td>\n",
       "      <td>Sí</td>\n",
       "      <td>Moderado</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518238</td>\n",
       "      <td>0.152951</td>\n",
       "      <td>0.233972</td>\n",
       "      <td>0.290081</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.154276</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>Viudo</td>\n",
       "      <td>158.681358</td>\n",
       "      <td>114.807668</td>\n",
       "      <td>27.634473</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Moderado</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087128</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.908137</td>\n",
       "      <td>0.561079</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.176676</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Casado</td>\n",
       "      <td>184.451263</td>\n",
       "      <td>60.217207</td>\n",
       "      <td>24.094841</td>\n",
       "      <td>No</td>\n",
       "      <td>Sí</td>\n",
       "      <td>No</td>\n",
       "      <td>Sedentario</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216952</td>\n",
       "      <td>0.110858</td>\n",
       "      <td>0.713157</td>\n",
       "      <td>1.180575</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Edad     Género Estado civil      Altura        Peso  \\\n",
       "0  76.596326       Otro      Soltero  153.681426   76.920289   \n",
       "1  79.795297       Otro       Casado  155.882307   66.743641   \n",
       "2  90.603394       Otro       Casado  176.481841  124.818134   \n",
       "3  22.154276   Femenino        Viudo  158.681358  114.807668   \n",
       "4  46.176676  Masculino       Casado  184.451263   60.217207   \n",
       "\n",
       "   Índice de masa corporal ¿Fuma actualmente? ¿Fumó en el pasado?  \\\n",
       "0                29.612895                 No                  Sí   \n",
       "1                 9.902543                 No                  No   \n",
       "2                27.248719                 Sí                  No   \n",
       "3                27.634473                 No                  No   \n",
       "4                24.094841                 No                  Sí   \n",
       "\n",
       "  ¿Consume alcohol frecuentemente? Nivel de actividad física  ... Asma_real  \\\n",
       "0                               No                  Moderado  ...  0.607766   \n",
       "1                               No                  Moderado  ...  0.127169   \n",
       "2                               Sí                  Moderado  ...  0.518238   \n",
       "3                               No                  Moderado  ...  0.087128   \n",
       "4                               No                Sedentario  ...  0.216952   \n",
       "\n",
       "  Cáncer_real Obesidad_real Depresión/Ansiedad_real  \\\n",
       "0    0.206125      0.185332                0.526435   \n",
       "1    0.464862      0.353357                1.062676   \n",
       "2    0.152951      0.233972                0.290081   \n",
       "3    0.016280      0.908137                0.561079   \n",
       "4    0.110858      0.713157                1.180575   \n",
       "\n",
       "  Enfermedad cardiovascular_bin Diabetes_bin Asma_bin Cáncer_bin Obesidad_bin  \\\n",
       "0                             1            0        1          0            0   \n",
       "1                             0            0        0          0            0   \n",
       "2                             1            0        1          0            0   \n",
       "3                             1            0        0          0            1   \n",
       "4                             1            0        0          0            1   \n",
       "\n",
       "  Depresión/Ansiedad_bin  \n",
       "0                      1  \n",
       "1                      1  \n",
       "2                      0  \n",
       "3                      1  \n",
       "4                      1  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# En este punto supongo que df ya está cargado:\n",
    "# df = pd.read_csv(\"health_data.csv\")\n",
    "\n",
    "# Diccionario con las medias y desviaciones estándar que nos dieron\n",
    "# para cada enfermedad. Lo uso para \"deshacer\" la estandarización.\n",
    "stats = {\n",
    "    \"Enfermedad cardiovascular\": {\"mean\": 0.303130, \"std\": 0.514926},\n",
    "    \"Diabetes\": {\"mean\": 0.205536, \"std\": 0.450558},\n",
    "    \"Asma\": {\"mean\": 0.150921, \"std\": 0.399708},\n",
    "    \"Cáncer\": {\"mean\": 0.105589, \"std\": 0.342888},\n",
    "    \"Obesidad\": {\"mean\": 0.248486, \"std\": 0.483748},\n",
    "    \"Depresión/Ansiedad\": {\"mean\": 0.401899, \"std\": 0.545406}\n",
    "}\n",
    "\n",
    "# 1) Desestandarizo las columnas de enfermedades para volver a la escala original\n",
    "for col, info in stats.items():\n",
    "    # nueva columna *_real con el valor \"deshecho\" de la estandarización\n",
    "    df[col + \"_real\"] = df[col] * info[\"std\"] + info[\"mean\"]\n",
    "\n",
    "# 2) Paso cada enfermedad a binaria:\n",
    "#    1 si la probabilidad es mayor o igual a 0.5, 0 si es menor\n",
    "for col in stats:\n",
    "    df[col + \"_bin\"] = (df[col + \"_real\"] >= 0.5).astype(int)\n",
    "\n",
    "# Solo para chequear rápido cómo queda el dataframe\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326e0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me quedo con solo las columnas \"reales\" de las enfermedades\n",
    "disease_cols = [\n",
    "    \"Enfermedad cardiovascular_real\",\n",
    "    \"Diabetes_real\",\n",
    "    \"Asma_real\",\n",
    "    \"Cáncer_real\",\n",
    "    \"Obesidad_real\",\n",
    "    \"Depresión/Ansiedad_real\"\n",
    "]\n",
    "\n",
    "df_diseases = df[disease_cols].copy()\n",
    "\n",
    "# Guardo  en un CSV \n",
    "df_diseases.to_csv(\"diseases_only.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d642241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "\n",
    "pyreadr.write_rdata(\"diseases_only.RData\", df_diseases, \"diseases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fc116a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disease_combo</th>\n",
       "      <th>disease_combo_reduced</th>\n",
       "      <th>disease_class_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1, 0, 1, 0, 0, 1)</td>\n",
       "      <td>(1, 0, 1, 0, 0, 1)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 0, 0, 0, 0, 1)</td>\n",
       "      <td>(0, 0, 0, 0, 0, 1)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1, 0, 1, 0, 0, 0)</td>\n",
       "      <td>(1, 0, 1, 0, 0, 0)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1, 0, 0, 0, 1, 1)</td>\n",
       "      <td>(1, 0, 0, 0, 1, 1)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1, 0, 0, 0, 1, 1)</td>\n",
       "      <td>(1, 0, 0, 0, 1, 1)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        disease_combo disease_combo_reduced  disease_class_final\n",
       "0  (1, 0, 1, 0, 0, 1)    (1, 0, 1, 0, 0, 1)                    0\n",
       "1  (0, 0, 0, 0, 0, 1)    (0, 0, 0, 0, 0, 1)                    1\n",
       "2  (1, 0, 1, 0, 0, 0)    (1, 0, 1, 0, 0, 0)                    2\n",
       "3  (1, 0, 0, 0, 1, 1)    (1, 0, 0, 0, 1, 1)                    3\n",
       "4  (1, 0, 0, 0, 1, 1)    (1, 0, 0, 0, 1, 1)                    3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Columnas binarias de las 6 enfermedades (las que ya creaste)\n",
    "disease_bin_cols = [\n",
    "    \"Enfermedad cardiovascular_bin\",\n",
    "    \"Diabetes_bin\",\n",
    "    \"Asma_bin\",\n",
    "    \"Cáncer_bin\",\n",
    "    \"Obesidad_bin\",\n",
    "    \"Depresión/Ansiedad_bin\"\n",
    "]\n",
    "\n",
    "\n",
    "df[\"disease_combo\"] = df[disease_bin_cols].apply(lambda fila: tuple(fila.values), axis=1)\n",
    "\n",
    "\n",
    "combo_counts = df[\"disease_combo\"].value_counts()\n",
    "\n",
    "min_count = 30   # si quieres menos clases, sube este número\n",
    "\n",
    "combos_comunes = combo_counts[combo_counts >= min_count].index\n",
    "\n",
    "def agrupar_combos(combo):\n",
    "    if combo in combos_comunes:\n",
    "        return combo\n",
    "    else:\n",
    "        return \"OTRAS\"\n",
    "\n",
    "# Versión reducida de la combinación, con las raras agrupadas\n",
    "df[\"disease_combo_reduced\"] = df[\"disease_combo\"].apply(agrupar_combos)\n",
    "\n",
    "# Ahora paso estas etiquetas (tuplas + \"OTRAS\") a números 0,1,2,...\n",
    "etiquetas_unicas = list(df[\"disease_combo_reduced\"].unique())\n",
    "label_to_id = {etq: i for i, etq in enumerate(etiquetas_unicas)}\n",
    "\n",
    "df[\"disease_class_final\"] = df[\"disease_combo_reduced\"].map(label_to_id)\n",
    "\n",
    "df[[\"disease_combo\", \"disease_combo_reduced\", \"disease_class_final\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdd297",
   "metadata": {},
   "source": [
    "## Introducción a los modelos basados en redes neuronales\n",
    "\n",
    "A partir del análisis exploratorio de los datos de salud, el objetivo de esta parte del trabajo fue\n",
    "construir y comparar distintos modelos predictivos capaces de identificar la presencia de\n",
    "enfermedades (o combinaciones de ellas) a partir de variables demográficas, de estilo de vida y\n",
    "antecedentes clínicos.  \n",
    "\n",
    "Dado que el conjunto de datos es relativamente grande y las relaciones entre variables pueden ser\n",
    "no lineales, optamos por utilizar **redes neuronales artificiales** como modelo base. A partir de\n",
    "esta idea probamos varias variantes:\n",
    "\n",
    "1. **Red neuronal multiclase básica**  \n",
    "   Primero definimos un problema de clasificación multiclase donde la variable objetivo\n",
    "   `disease_class_final` agrupa las distintas combinaciones de enfermedades.  \n",
    "   Entrenamos una red densa con dos capas ocultas (64 y 32 neuronas con activación ReLU y\n",
    "   dropout), salida softmax y pérdida `categorical_crossentropy`. Este modelo nos sirve como\n",
    "   punto de partida para evaluar qué tan bien se pueden distinguir las combinaciones de\n",
    "   enfermedades usando solo las variables de entrada disponibles.\n",
    "\n",
    "2. **Bagging con redes neuronales**  \n",
    "   Para intentar reducir la varianza del modelo y ganar estabilidad, construimos un\n",
    "   **ensemble** mediante bagging: entrenamos varias redes con la misma arquitectura sobre\n",
    "   distintos bootstraps del conjunto de entrenamiento y promediamos sus predicciones.  \n",
    "   Probamos tanto un bagging “simple” como una variante con bootstraps balanceados por clase,\n",
    "   con la idea de darle más presencia a las clases minoritarias durante el entrenamiento.\n",
    "\n",
    "3. **Red neuronal multiclase con pesos de clase**  \n",
    "   Dado el fuerte desbalance entre combinaciones de enfermedades, incorporamos también un\n",
    "   modelo multiclase con **`class_weight`**, penalizando más los errores en las clases raras.\n",
    "   Este modelo permite comprobar hasta qué punto se puede mejorar el F1-macro ajustando el\n",
    "   coste de las clases en la función de pérdida, sin cambiar la arquitectura de la red.\n",
    "\n",
    "4. **Red neuronal multi-etiqueta (una salida por enfermedad)**  \n",
    "   Finalmente replanteamos el problema como **clasificación multi-etiqueta**, prediciendo en\n",
    "   paralelo seis variables binarias (`*_bin`) que indican la presencia de enfermedad\n",
    "   cardiovascular, diabetes, asma, cáncer, obesidad y depresión/ansiedad.  \n",
    "   Para ello usamos la misma estructura de red, pero con una capa de salida de seis neuronas\n",
    "   sigmoides y pérdida `binary_crossentropy`, lo que nos permite analizar el rendimiento\n",
    "   individual por enfermedad y no solo por combinación.\n",
    "\n",
    "En las secciones siguientes describimos en detalle cada uno de estos modelos, los parámetros\n",
    "utilizados (arquitectura, épocas, tamaño de batch, etc.) y las métricas obtenidas (accuracy,\n",
    "F1-macro y F1 por enfermedad). Esto nos permite entender no solo qué modelo funciona mejor, sino\n",
    "también hasta qué punto el **desbalance de clases y la información disponible en las variables**\n",
    "limitan el rendimiento de las redes neuronales en este problema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e99e1",
   "metadata": {},
   "source": [
    "# Preparar X e y y seguir con la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db55c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (20000, 50)\n",
      "Clases distintas en y: 34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# 1) columnas de enfermedades\n",
    "disease_bin_cols = [\n",
    "    \"Enfermedad cardiovascular_bin\",\n",
    "    \"Diabetes_bin\",\n",
    "    \"Asma_bin\",\n",
    "    \"Cáncer_bin\",\n",
    "    \"Obesidad_bin\",\n",
    "    \"Depresión/Ansiedad_bin\"\n",
    "]\n",
    "\n",
    "disease_cont_cols = [\n",
    "    \"Enfermedad cardiovascular\",\n",
    "    \"Diabetes\",\n",
    "    \"Asma\",\n",
    "    \"Cáncer\",\n",
    "    \"Obesidad\",\n",
    "    \"Depresión/Ansiedad\"\n",
    "]\n",
    "\n",
    "disease_real_cols = [c + \"_real\" for c in disease_cont_cols]\n",
    "\n",
    "target_col = \"disease_class_final\"\n",
    "\n",
    "cols_a_sacar = (\n",
    "    disease_bin_cols\n",
    "    + disease_cont_cols\n",
    "    + disease_real_cols\n",
    "    + [\"disease_combo\", \"disease_combo_reduced\",\n",
    "       \"disease_class\", \"disease_class_reduced\", \"disease_class_final\"]\n",
    ")\n",
    "\n",
    "cols_a_sacar = [c for c in cols_a_sacar if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=cols_a_sacar)\n",
    "y = df[target_col]\n",
    "\n",
    "print(\"Shape X:\", X.shape)\n",
    "print(\"Clases distintas en y:\", y.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8677027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas categóricas: ['Género', 'Estado civil', '¿Fuma actualmente?', '¿Fumó en el pasado?', '¿Consume alcohol frecuentemente?', 'Nivel de actividad física', '¿Tiene una dieta equilibrada?', '¿Consume frutas y verduras diariamente?', 'Frecuencia de consumo de comida rápida', '¿Duerme al menos 7 horas por noche?', '¿Experimenta estrés con frecuencia?', '¿Tiene antecedentes de hipertensión en la familia?', '¿Tiene antecedentes de diabetes en la familia?', '¿Tiene antecedentes de cáncer en la familia?', '¿Tiene antecedentes de enfermedades cardiovasculares en la familia?', '¿Tiene antecedentes de problemas de tiroides en la familia?', 'Frecuencia de ejercicio físico semanal', '¿Toma medicamentos regularmente?', 'Nivel de colesterol', 'Nivel de triglicéridos', 'Nivel de glucosa en sangre', 'Presión arterial', 'Consumo de sal en la dieta', '¿Tiene antecedentes de obesidad en la familia?', '¿Tiene antecedentes de asma?', '¿Padece de alguna alergia?', '¿Ha tenido infecciones respiratorias frecuentes en el último año?', '¿Tiene dificultades para respirar durante el ejercicio?', '¿Toma suplementos vitamínicos?', 'Nivel de azúcar en la dieta', '¿Ha experimentado pérdida de peso no intencionada?', '¿Ha tenido dolor en el pecho recientemente?', '¿Experimenta dolor en las articulaciones?', '¿Tiene problemas para conciliar el sueño?', '¿Ha tenido tos persistente en los últimos 3 meses?', 'Nivel de estrés laboral o académico', '¿Ha experimentado depresión o ansiedad?', '¿Consume alimentos procesados frecuentemente?', '¿Padece de insomnio?', 'Frecuencia de chequeos médicos', '¿Tiene problemas digestivos frecuentes?', '¿Ha tenido infecciones frecuentes?', '¿Sufre de problemas de visión?', '¿Tiene problemas de audición?', '¿Ha sufrido de fracturas óseas en el pasado?', 'Nivel de satisfacción con la vida']\n",
      "Shape X después de dummies: (20000, 108)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "print(\"Columnas categóricas:\", list(cat_cols))\n",
    "\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=False, dtype=int)\n",
    "print(\"Shape X después de dummies:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7cb8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1369ef3",
   "metadata": {},
   "source": [
    "### Configuración del modelo 1 de red neuronal\n",
    "\n",
    "Para el modelo de redes neuronales usamos la siguiente arquitectura y parámetros:\n",
    "\n",
    "- **Arquitectura `Sequential`**  \n",
    "  Elegimos un modelo `Sequential` porque nuestro problema es un flujo sencillo de entrada → salida, sin conexiones raras ni múltiples entradas. Esto hace el código más simple y suficiente para una tarea de clasificación.\n",
    "\n",
    "- **Capas densas de 64 y 32 neuronas con `relu`**  \n",
    "  Usamos dos capas ocultas:\n",
    "  - Primera capa: `Dense(64, activation = \"relu\")`  \n",
    "  - Segunda capa: `Dense(32, activation = \"relu\")`  \n",
    "  El tamaño 64–32 es un compromiso entre capacidad y sobreajuste: da suficientes neuronas para capturar relaciones no lineales entre las variables, pero sin ser tan grande como para memorizar el dataset. La activación `relu` se usa porque suele converger rápido y funciona bien en problemas de clasificación tabular.\n",
    "\n",
    "- **Capas `Dropout(0.3)`**  \n",
    "  Después de cada capa densa añadimos `Dropout(0.3)`, es decir, apagamos aleatoriamente el 30% de las neuronas en cada paso de entrenamiento. Esto ayuda a **reducir el sobreajuste**, obligando al modelo a no depender demasiado de unas pocas neuronas y a generalizar mejor al conjunto de test.\n",
    "\n",
    "- **Capa de salida con `num_classes` neuronas y `softmax`**  \n",
    "  La última capa tiene tantas neuronas como clases (`num_classes`) y usa activación `softmax`.  \n",
    "  Esto nos da, para cada observación, una distribución de probabilidades sobre todas las clases posibles y es el estándar en problemas de **clasificación multiclase**.\n",
    "\n",
    "- **Codificación one-hot del objetivo (`to_categorical`)**  \n",
    "  Convertimos `y_train` e `y_test` a formato one-hot con `to_categorical`. Así cada clase se representa como un vector binario, compatible con la salida `softmax` y la pérdida `categorical_crossentropy`.\n",
    "\n",
    "- **Función de pérdida: `categorical_crossentropy`**  \n",
    "  Usamos `categorical_crossentropy` porque es la función de pérdida habitual cuando:\n",
    "  1) la variable objetivo tiene más de dos clases, y  \n",
    "  2) la salida del modelo es una probabilidad por clase (softmax).\n",
    "\n",
    "- **Optimizador `adam`**  \n",
    "  Elegimos `adam` porque es un optimizador robusto y automático: ajusta la tasa de aprendizaje internamente y suele dar buen rendimiento sin mucho ajuste fino, lo cual es práctico para un proyecto aplicado como este.\n",
    "\n",
    "- **Número de épocas: `epochs = 30`**  \n",
    "  Entrenamos durante 30 épocas como punto medio: suficiente para que el modelo aprenda los patrones principales sin entrenar “eternamente”. Además, al usar `validation_split = 0.2` podemos ver si la pérdida de validación deja de mejorar y, si hiciera falta, podríamos reducir o aumentar este valor.\n",
    "\n",
    "- **Tamaño de batch: `batch_size = 128`**  \n",
    "  Un batch de 128 observaciones equilibra estabilidad en el gradiente y tiempo de cómputo:  \n",
    "  batches muy pequeños hacen el entrenamiento ruidoso; batches enormes pueden ser más lentos y consumir más memoria. 128 es un valor estándar que funciona bien en la práctica.\n",
    "\n",
    "- **`validation_split = 0.2`**  \n",
    "  Reservamos el 20% del conjunto de entrenamiento como validación interna. Esto nos permite monitorizar si el modelo empieza a sobreajustar (cuando la pérdida de validación empeora mientras la de entrenamiento mejora) sin tocar todavía el conjunto de test.\n",
    "\n",
    "En resumen, elegimos una arquitectura relativamente pequeña (64–32 neuronas) con `relu` y `dropout` como compromiso entre capacidad de modelar relaciones no lineales y control del sobreajuste, utilizando los hiperparámetros estándar (`adam`, `categorical_crossentropy`, `batch_size = 128`, `epochs = 30`) para un problema de clasificación multiclase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af7fc0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,976</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,122</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m6,976\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)             │         \u001b[38;5;34m1,122\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,178</span> (39.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,178\u001b[0m (39.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,178</span> (39.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,178\u001b[0m (39.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_features = X_train_scaled.shape[1]\n",
    "num_classes  = y_train.nunique()\n",
    "\n",
    "# Paso y a one-hot para usar softmax\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat  = to_categorical(y_test,  num_classes=num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation=\"relu\", input_shape=(num_features,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61de2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 - 1s - 15ms/step - accuracy: 0.1337 - loss: 3.2658 - val_accuracy: 0.1672 - val_loss: 2.9260\n",
      "Epoch 2/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1620 - loss: 2.9604 - val_accuracy: 0.1722 - val_loss: 2.8599\n",
      "Epoch 3/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1756 - loss: 2.9041 - val_accuracy: 0.1756 - val_loss: 2.8447\n",
      "Epoch 4/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1804 - loss: 2.8655 - val_accuracy: 0.1769 - val_loss: 2.8401\n",
      "Epoch 5/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1830 - loss: 2.8497 - val_accuracy: 0.1791 - val_loss: 2.8367\n",
      "Epoch 6/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1847 - loss: 2.8357 - val_accuracy: 0.1713 - val_loss: 2.8370\n",
      "Epoch 7/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1814 - loss: 2.8212 - val_accuracy: 0.1741 - val_loss: 2.8349\n",
      "Epoch 8/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1827 - loss: 2.8163 - val_accuracy: 0.1787 - val_loss: 2.8360\n",
      "Epoch 9/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1900 - loss: 2.8036 - val_accuracy: 0.1694 - val_loss: 2.8356\n",
      "Epoch 10/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1872 - loss: 2.8020 - val_accuracy: 0.1794 - val_loss: 2.8357\n",
      "Epoch 11/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1899 - loss: 2.8000 - val_accuracy: 0.1769 - val_loss: 2.8347\n",
      "Epoch 12/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1896 - loss: 2.7985 - val_accuracy: 0.1734 - val_loss: 2.8379\n",
      "Epoch 13/30\n",
      "100/100 - 0s - 2ms/step - accuracy: 0.1895 - loss: 2.7920 - val_accuracy: 0.1716 - val_loss: 2.8415\n",
      "Epoch 14/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1892 - loss: 2.7885 - val_accuracy: 0.1766 - val_loss: 2.8392\n",
      "Epoch 15/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1912 - loss: 2.7852 - val_accuracy: 0.1803 - val_loss: 2.8392\n",
      "Epoch 16/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1888 - loss: 2.7824 - val_accuracy: 0.1753 - val_loss: 2.8411\n",
      "Epoch 17/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1903 - loss: 2.7763 - val_accuracy: 0.1813 - val_loss: 2.8386\n",
      "Epoch 18/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1937 - loss: 2.7753 - val_accuracy: 0.1784 - val_loss: 2.8401\n",
      "Epoch 19/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1924 - loss: 2.7691 - val_accuracy: 0.1709 - val_loss: 2.8385\n",
      "Epoch 20/30\n",
      "100/100 - 0s - 2ms/step - accuracy: 0.1905 - loss: 2.7681 - val_accuracy: 0.1791 - val_loss: 2.8410\n",
      "Epoch 21/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1913 - loss: 2.7674 - val_accuracy: 0.1809 - val_loss: 2.8394\n",
      "Epoch 22/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1946 - loss: 2.7615 - val_accuracy: 0.1813 - val_loss: 2.8425\n",
      "Epoch 23/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1942 - loss: 2.7627 - val_accuracy: 0.1803 - val_loss: 2.8433\n",
      "Epoch 24/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1956 - loss: 2.7578 - val_accuracy: 0.1769 - val_loss: 2.8442\n",
      "Epoch 25/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1937 - loss: 2.7599 - val_accuracy: 0.1781 - val_loss: 2.8454\n",
      "Epoch 26/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1940 - loss: 2.7573 - val_accuracy: 0.1769 - val_loss: 2.8465\n",
      "Epoch 27/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1948 - loss: 2.7486 - val_accuracy: 0.1753 - val_loss: 2.8464\n",
      "Epoch 28/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1959 - loss: 2.7469 - val_accuracy: 0.1809 - val_loss: 2.8488\n",
      "Epoch 29/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1931 - loss: 2.7436 - val_accuracy: 0.1825 - val_loss: 2.8465\n",
      "Epoch 30/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1970 - loss: 2.7520 - val_accuracy: 0.1797 - val_loss: 2.8486\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_cat,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8443392",
   "metadata": {},
   "source": [
    "### Evolución del entrenamiento\n",
    "\n",
    "Durante las 30 épocas de entrenamiento se observa:\n",
    "\n",
    "- La **accuracy de entrenamiento** parte alrededor de 0.13 y sube muy poco, hasta ~0.19.\n",
    "- La **accuracy de validación** se mantiene también baja y bastante estable, en el rango 0.16–0.18.\n",
    "- La **loss** y la **val_loss** bajan ligeramente al principio y luego se estabilizan, sin señales claras de sobreajuste (no hay una separación grande entre loss de train y de validación).\n",
    "\n",
    "Esto sugiere que el modelo **no está aprendiendo patrones muy fuertes** a partir de las variables disponibles: el rendimiento se estanca pronto y nunca llega a valores altos de exactitud. Es decir, el problema es realmente difícil si no le damos al modelo las enfermedades originales como entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bc003e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step\n",
      "F1 macro (Red Neuronal): 0.01417706885474748\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        50\n",
      "           1       0.19      0.84      0.31       747\n",
      "           2       0.00      0.00      0.00        44\n",
      "           3       0.00      0.00      0.00       133\n",
      "           4       0.00      0.00      0.00        63\n",
      "           5       0.16      0.17      0.16       635\n",
      "           6       1.00      0.00      0.01       329\n",
      "           7       0.00      0.00      0.00        46\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00        10\n",
      "          10       0.00      0.00      0.00       190\n",
      "          11       0.00      0.00      0.00       261\n",
      "          12       0.00      0.00      0.00         6\n",
      "          13       0.00      0.00      0.00        75\n",
      "          14       0.00      0.00      0.00        32\n",
      "          15       0.00      0.00      0.00        29\n",
      "          16       0.00      0.00      0.00        89\n",
      "          17       0.12      0.00      0.01       380\n",
      "          18       0.00      0.00      0.00       146\n",
      "          19       0.00      0.00      0.00        58\n",
      "          20       0.00      0.00      0.00       101\n",
      "          21       0.00      0.00      0.00        21\n",
      "          22       0.00      0.00      0.00       207\n",
      "          23       0.00      0.00      0.00        89\n",
      "          24       0.00      0.00      0.00        30\n",
      "          25       0.00      0.00      0.00        29\n",
      "          26       0.00      0.00      0.00        12\n",
      "          27       0.00      0.00      0.00        12\n",
      "          28       0.00      0.00      0.00        94\n",
      "          29       0.00      0.00      0.00        13\n",
      "          30       0.00      0.00      0.00        22\n",
      "          31       0.00      0.00      0.00         6\n",
      "          32       0.00      0.00      0.00         9\n",
      "          33       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.18      4000\n",
      "   macro avg       0.04      0.03      0.01      4000\n",
      "weighted avg       0.15      0.18      0.08      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred_class = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "print(\"F1 macro (Red Neuronal):\",\n",
    "      f1_score(y_test, y_pred_class, average=\"macro\"))\n",
    "\n",
    "print(classification_report(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9022b5",
   "metadata": {},
   "source": [
    "### Métricas finales en el conjunto de test\n",
    "\n",
    "El *classification report* sobre el conjunto de test muestra:\n",
    "\n",
    "- **Accuracy global ≈ 0.18**, lo que significa que el modelo acierta aproximadamente un 18% de los casos.\n",
    "- **F1 macro ≈ 0.01**, muy bajo: indica que, en promedio, las clases se predicen mal.  \n",
    "  La red tiende a concentrarse en unas pocas clases frecuentes y prácticamente ignora las raras.\n",
    "- En las métricas por clase se ve que:\n",
    "  - La clase más frecuente (por ejemplo la clase 1) tiene un *recall* alto (≈0.84), pero una precisión baja (≈0.19): el modelo casi siempre “se tira” a esa clase y acierta solo cuando realmente corresponde.\n",
    "  - Para muchas clases minoritarias, la precisión y el recall son 0: el modelo casi nunca las predice.\n",
    "\n",
    "En conclusión, después de eliminar la fuga de información (las columnas de enfermedades) la red neuronal **pierde la performance perfecta que tenía antes** y pasa a tener un rendimiento modesto, especialmente en las clases poco frecuentes. Esto es coherente con un problema desbalanceado y con variables predictoras que no determinan de forma directa la combinación de enfermedades. Sirve también para mostrar que, sin hacer trampa con las variables de entrada, el modelo tiene limitaciones claras y habría que explorar más técnicas (re-balanceo, otras arquitecturas, más features) si se quisiera mejorar su F1 macro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47986b",
   "metadata": {},
   "source": [
    "# Baggin con Red Neuronal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e80e5",
   "metadata": {},
   "source": [
    "### Bagging con redes neuronales\n",
    "\n",
    "Además del modelo de red neuronal individual, probamos un esquema de **bagging** (Bootstrap Aggregating) usando varias redes neuronales con la misma arquitectura.\n",
    "\n",
    "La idea del bagging es reducir la **varianza** del modelo: en lugar de entrenar una única red sobre todo el conjunto de entrenamiento, entrenamos varias redes sobre diferentes muestras bootstrap y luego promediamos sus predicciones.\n",
    "\n",
    "#### Arquitectura de cada red del ensemble\n",
    "\n",
    "Primero definimos una función `crear_modelo_nn()` que construye una red con la misma estructura que el modelo base:\n",
    "\n",
    "- Capa densa de **64 neuronas** con activación *ReLU*.\n",
    "- Capa de **Dropout(0.3)** para reducir sobreajuste.\n",
    "- Segunda capa densa de **32 neuronas** con activación *ReLU*.\n",
    "- Otra capa de **Dropout(0.3)**.\n",
    "- Capa de salida con `num_classes` neuronas y activación *softmax* (una probabilidad por cada combinación de enfermedades).\n",
    "\n",
    "La red se compila con:\n",
    "\n",
    "- **Optimizador:** `adam`, por su buen comportamiento general sin mucho ajuste fino.\n",
    "- **Pérdida:** `categorical_crossentropy`, apropiada para clasificación multiclase con salida *softmax*.\n",
    "- **Métrica:** `accuracy` para monitorizar el entrenamiento.\n",
    "\n",
    "De esta forma todas las redes del ensemble comparten la misma arquitectura y difieren solo en los datos concretos sobre los que se entrenan.\n",
    "\n",
    "#### Procedimiento de bagging\n",
    "\n",
    "Para el bagging usamos los siguientes parámetros:\n",
    "\n",
    "- **`n_models = 5`**  \n",
    "  Entrenamos 5 redes distintas. Es un número moderado que permite ver el efecto del ensamble sin disparar el tiempo de cómputo. Si se quisiera un ensemble más fuerte se podría aumentar este valor.\n",
    "\n",
    "- **`epocas_por_modelo = 20`**  \n",
    "  Cada red se entrena durante 20 épocas (algo menos que el modelo individual) para mantener razonable el tiempo total de entrenamiento, ya que ahora hay que entrenar varios modelos.\n",
    "\n",
    "El procedimiento dentro del bucle es:\n",
    "\n",
    "1. **Muestreo bootstrap:**  \n",
    "   Para cada modelo generamos una muestra bootstrap del conjunto de entrenamiento (`X_train_scaled`, `y_train`).  \n",
    "   Esto significa que seleccionamos aleatoriamente, con reemplazo, tantas filas como el tamaño del train original. Cada red ve una versión ligeramente distinta de los datos.\n",
    "\n",
    "2. **Codificación one-hot del objetivo:**  \n",
    "   Convertimos `y_boot` a formato one-hot (`y_boot_cat`) con `to_categorical`, igual que en el modelo base, para poder usar *softmax* + `categorical_crossentropy`.\n",
    "\n",
    "3. **Entrenamiento del modelo:**  \n",
    "   Creamos un nuevo modelo con `crear_modelo_nn()` y lo entrenamos sobre el bootstrap correspondiente (`X_boot`, `y_boot_cat`) durante `epocas_por_modelo` épocas, con `batch_size = 128`.\n",
    "\n",
    "4. **Almacenamiento de modelos:**  \n",
    "   Cada modelo entrenado se guarda en la lista `modelos_bagging` para usarlo después en la fase de predicción.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f36537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_features = X_train_scaled.shape[1]\n",
    "num_classes  = y_train.nunique()\n",
    "\n",
    "def crear_modelo_nn():\n",
    "    \"\"\"\n",
    "    Arma y compila una red neuronal con la arquitectura que queremos usar\n",
    "    en cada modelo del bagging.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation=\"relu\", input_shape=(num_features,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cb8482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,976</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,122</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m6,976\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)             │         \u001b[38;5;34m1,122\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,178</span> (39.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,178\u001b[0m (39.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,178</span> (39.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,178\u001b[0m (39.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 - 2s - 16ms/step - accuracy: 0.1268 - loss: 3.2186 - val_accuracy: 0.1706 - val_loss: 2.9251\n",
      "Epoch 2/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1697 - loss: 2.9596 - val_accuracy: 0.1775 - val_loss: 2.8684\n",
      "Epoch 3/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1705 - loss: 2.8996 - val_accuracy: 0.1719 - val_loss: 2.8531\n",
      "Epoch 4/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1819 - loss: 2.8667 - val_accuracy: 0.1797 - val_loss: 2.8455\n",
      "Epoch 5/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1852 - loss: 2.8443 - val_accuracy: 0.1816 - val_loss: 2.8402\n",
      "Epoch 6/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1882 - loss: 2.8331 - val_accuracy: 0.1819 - val_loss: 2.8405\n",
      "Epoch 7/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1858 - loss: 2.8205 - val_accuracy: 0.1838 - val_loss: 2.8370\n",
      "Epoch 8/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1854 - loss: 2.8165 - val_accuracy: 0.1834 - val_loss: 2.8381\n",
      "Epoch 9/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1886 - loss: 2.8080 - val_accuracy: 0.1853 - val_loss: 2.8344\n",
      "Epoch 10/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1873 - loss: 2.8074 - val_accuracy: 0.1834 - val_loss: 2.8336\n",
      "Epoch 11/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1878 - loss: 2.7997 - val_accuracy: 0.1831 - val_loss: 2.8338\n",
      "Epoch 12/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1920 - loss: 2.7940 - val_accuracy: 0.1822 - val_loss: 2.8338\n",
      "Epoch 13/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1924 - loss: 2.7938 - val_accuracy: 0.1816 - val_loss: 2.8352\n",
      "Epoch 14/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1905 - loss: 2.7881 - val_accuracy: 0.1813 - val_loss: 2.8362\n",
      "Epoch 15/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1882 - loss: 2.7800 - val_accuracy: 0.1813 - val_loss: 2.8379\n",
      "Epoch 16/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1923 - loss: 2.7769 - val_accuracy: 0.1762 - val_loss: 2.8371\n",
      "Epoch 17/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1858 - loss: 2.7792 - val_accuracy: 0.1797 - val_loss: 2.8393\n",
      "Epoch 18/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1932 - loss: 2.7769 - val_accuracy: 0.1816 - val_loss: 2.8382\n",
      "Epoch 19/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1926 - loss: 2.7742 - val_accuracy: 0.1831 - val_loss: 2.8397\n",
      "Epoch 20/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1955 - loss: 2.7730 - val_accuracy: 0.1781 - val_loss: 2.8400\n",
      "Epoch 21/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1932 - loss: 2.7692 - val_accuracy: 0.1794 - val_loss: 2.8386\n",
      "Epoch 22/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1924 - loss: 2.7706 - val_accuracy: 0.1806 - val_loss: 2.8403\n",
      "Epoch 23/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1929 - loss: 2.7658 - val_accuracy: 0.1794 - val_loss: 2.8399\n",
      "Epoch 24/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1959 - loss: 2.7636 - val_accuracy: 0.1819 - val_loss: 2.8401\n",
      "Epoch 25/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1914 - loss: 2.7624 - val_accuracy: 0.1813 - val_loss: 2.8401\n",
      "Epoch 26/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1945 - loss: 2.7552 - val_accuracy: 0.1825 - val_loss: 2.8416\n",
      "Epoch 27/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1940 - loss: 2.7570 - val_accuracy: 0.1806 - val_loss: 2.8414\n",
      "Epoch 28/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1911 - loss: 2.7511 - val_accuracy: 0.1831 - val_loss: 2.8415\n",
      "Epoch 29/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1941 - loss: 2.7539 - val_accuracy: 0.1806 - val_loss: 2.8424\n",
      "Epoch 30/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1970 - loss: 2.7542 - val_accuracy: 0.1816 - val_loss: 2.8417\n"
     ]
    }
   ],
   "source": [
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat  = to_categorical(y_test,  num_classes=num_classes)\n",
    "\n",
    "model_base = crear_modelo_nn()\n",
    "model_base.summary()\n",
    "\n",
    "history = model_base.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_cat,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab20dce",
   "metadata": {},
   "source": [
    "#### Entrenamiento de las redes dentro del bagging\n",
    "\n",
    "Cada uno de los modelos del ensemble utiliza la misma arquitectura:\n",
    "\n",
    "- Capa densa de 64 neuronas con activación *ReLU*.\n",
    "- Capa de *Dropout(0.3)* para regularizar.\n",
    "- Capa densa de 32 neuronas con activación *ReLU*.\n",
    "- Segunda capa de *Dropout(0.3)*.\n",
    "- Capa de salida con 34 neuronas y activación *softmax*, una por cada clase de `disease_class_final`.\n",
    "\n",
    "El resumen del modelo indica un total de **10.178 parámetros entrenables**, por lo que cada red del bagging tiene una capacidad moderada: suficiente para capturar relaciones no lineales, pero sin llegar a ser un modelo gigante.\n",
    "\n",
    "Durante el entrenamiento (30 épocas) se observa que:\n",
    "\n",
    "- La **accuracy de entrenamiento** empieza alrededor de 0.12 y sube lentamente hasta ~0.19.\n",
    "- La **accuracy de validación** se mantiene en torno a 0.17–0.18 durante casi todas las épocas.\n",
    "- La **pérdida de entrenamiento y de validación** disminuyen ligeramente al principio y luego se estabilizan, sin que aparezca una separación grande entre ambas.\n",
    "\n",
    "Este comportamiento es muy parecido al del modelo de red neuronal individual: el modelo aprende algo de estructura en los datos, pero no consigue una precisión alta y no parece haber sobreajuste fuerte. En otras palabras, incluso entrenando varias redes con esta arquitectura, el problema sigue siendo difícil de resolver únicamente a partir de las variables de entrada disponibles (sin usar directamente las enfermedades originales).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "965a09b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo 1/5 del bagging...\n",
      "\n",
      "Entrenando modelo 2/5 del bagging...\n",
      "\n",
      "Entrenando modelo 3/5 del bagging...\n",
      "\n",
      "Entrenando modelo 4/5 del bagging...\n",
      "\n",
      "Entrenando modelo 5/5 del bagging...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "n_models = 5\n",
    "epocas_por_modelo = 20\n",
    "modelos_bagging = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    print(f\"\\nEntrenando modelo {i+1}/{n_models} del bagging...\")\n",
    "\n",
    "    # Bootstrap sobre el train\n",
    "    indices = np.random.choice(len(X_train_scaled), size=len(X_train_scaled), replace=True)\n",
    "    X_boot = X_train_scaled[indices]\n",
    "    y_boot = y_train.iloc[indices]\n",
    "\n",
    "    # One-hot SOLO para este bootstrap\n",
    "    y_boot_cat = to_categorical(y_boot, num_classes=num_classes)\n",
    "\n",
    "    # Modelo nuevo para este bootstrap\n",
    "    m = crear_modelo_nn()\n",
    "    m.fit(\n",
    "        X_boot,\n",
    "        y_boot_cat,\n",
    "        epochs=epocas_por_modelo,\n",
    "        batch_size=128,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    modelos_bagging.append(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "909e047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo con modelo 1/5...\n",
      "Prediciendo con modelo 2/5...\n",
      "Prediciendo con modelo 3/5...\n",
      "Prediciendo con modelo 4/5...\n",
      "Prediciendo con modelo 5/5...\n",
      "F1 macro (bagging de redes): 0.013939310899704663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        50\n",
      "           1       0.19      0.80      0.31       747\n",
      "           2       0.00      0.00      0.00        44\n",
      "           3       0.00      0.00      0.00       133\n",
      "           4       0.00      0.00      0.00        63\n",
      "           5       0.15      0.19      0.17       635\n",
      "           6       0.00      0.00      0.00       329\n",
      "           7       0.00      0.00      0.00        46\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00        10\n",
      "          10       0.00      0.00      0.00       190\n",
      "          11       0.00      0.00      0.00       261\n",
      "          12       0.00      0.00      0.00         6\n",
      "          13       0.00      0.00      0.00        75\n",
      "          14       0.00      0.00      0.00        32\n",
      "          15       0.00      0.00      0.00        29\n",
      "          16       0.00      0.00      0.00        89\n",
      "          17       0.00      0.00      0.00       380\n",
      "          18       0.00      0.00      0.00       146\n",
      "          19       0.00      0.00      0.00        58\n",
      "          20       0.00      0.00      0.00       101\n",
      "          21       0.00      0.00      0.00        21\n",
      "          22       0.00      0.00      0.00       207\n",
      "          23       0.00      0.00      0.00        89\n",
      "          24       0.00      0.00      0.00        30\n",
      "          25       0.00      0.00      0.00        29\n",
      "          26       0.00      0.00      0.00        12\n",
      "          27       0.00      0.00      0.00        12\n",
      "          28       0.00      0.00      0.00        94\n",
      "          29       0.00      0.00      0.00        13\n",
      "          30       0.00      0.00      0.00        22\n",
      "          31       0.00      0.00      0.00         6\n",
      "          32       0.00      0.00      0.00         9\n",
      "          33       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.18      4000\n",
      "   macro avg       0.01      0.03      0.01      4000\n",
      "weighted avg       0.06      0.18      0.08      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Predicciones de cada red\n",
    "preds_proba = []\n",
    "\n",
    "for i, m in enumerate(modelos_bagging):\n",
    "    print(f\"Prediciendo con modelo {i+1}/{n_models}...\")\n",
    "    p = m.predict(X_test_scaled, verbose=0)\n",
    "    preds_proba.append(p)\n",
    "\n",
    "# Promedio de probabilidades entre todos los modelos\n",
    "preds_proba = np.array(preds_proba)          # shape: (n_models, n_muestras, num_classes)\n",
    "mean_proba = preds_proba.mean(axis=0)        # shape: (n_muestras, num_classes)\n",
    "\n",
    "y_pred_bagging_nn = mean_proba.argmax(axis=1)\n",
    "\n",
    "print(\"F1 macro (bagging de redes):\",\n",
    "      f1_score(y_test, y_pred_bagging_nn, average=\"macro\"))\n",
    "\n",
    "print(classification_report(y_test, y_pred_bagging_nn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe6516",
   "metadata": {},
   "source": [
    "### Resultados del bagging con redes neuronales\n",
    "\n",
    "Al evaluar el ensemble de 5 redes sobre el conjunto de test obtuvimos:\n",
    "\n",
    "- **Accuracy global:** ≈ 0,18  \n",
    "- **F1 macro:** ≈ 0,014  \n",
    "- **Precision macro:** ≈ 0,01  \n",
    "- **Recall macro:** ≈ 0,03  \n",
    "\n",
    "#### Comportamiento por clase\n",
    "\n",
    "El *classification report* muestra que los errores no se reparten de forma uniforme:\n",
    "\n",
    "- La **clase 1**, que es la combinación de enfermedades más frecuente (747 casos en el test), es prácticamente la única que el modelo identifica con cierta calidad:  \n",
    "  - *Precision* ≈ 0,19  \n",
    "  - *Recall* ≈ 0,80  \n",
    "  - *F1-score* ≈ 0,31  \n",
    "\n",
    "  Es decir, el ensemble suele “apostar” por esta clase y acierta en muchos de los casos que realmente pertenecen a ella, pero también etiqueta como 1 bastantes observaciones de otras clases.\n",
    "\n",
    "- Para la mayoría de las **clases minoritarias** (0, 2, 3, 4, 6, 7, 8, …) la *precision*, el *recall* y el *F1-score* son prácticamente **0**.  \n",
    "  El modelo casi nunca las predice: concentra sus decisiones en unas pocas clases grandes.\n",
    "\n",
    "Como el **F1 macro** promedia por igual todas las clases, incluyendo aquellas en las que el rendimiento es nulo, el valor final cae a ≈ 0,014 a pesar de que la clase mayoritaria tiene un F1 razonable.\n",
    "\n",
    "#### Comparación con la red neuronal individual\n",
    "\n",
    "El bagging con redes neuronales **no mejora de forma relevante** el rendimiento respecto a la red neuronal individual: los valores de accuracy y F1 macro son prácticamente los mismos en ambos casos. Esto tiene sentido por varios motivos:\n",
    "\n",
    "1. **Misma información de entrada**  \n",
    "   Tanto la red individual como las redes del ensemble usan exactamente las mismas variables explicativas (sin las enfermedades originales). Si esas features no separan bien las clases, ningún método de ensamble va a “inventar” información nueva.\n",
    "\n",
    "2. **Misma arquitectura y mismo tipo de error**  \n",
    "   Todas las redes del bagging comparten la misma arquitectura (64–32 neuronas con ReLU y dropout). Como consecuencia, tienden a cometer **errores muy parecidos**: favorecen las clases más frecuentes y casi nunca detectan las raras.  \n",
    "   El bagging reduce la varianza entre modelos, pero aquí el problema principal es el **sesgo**: el modelo no tiene capacidad/información suficiente para distinguir bien todas las combinaciones.\n",
    "\n",
    "3. **Fuerte desbalance de clases**  \n",
    "   El dataset está muy desbalanceado: algunas combinaciones de enfermedades aparecen muchas veces y otras muy pocas. El ensemble aprende a priorizar las clases grandes (de ahí el recall alto de la clase 1) y sacrifica completamente las minoritarias, que quedan con F1 ≈ 0.\n",
    "\n",
    "En conjunto, este experimento muestra que el límite del rendimiento no está tanto en usar una red sola o un ensemble, sino en la propia estructura del problema y en el desbalance extremo de las clases. Con las variables disponibles, resulta difícil lograr un buen F1 macro para todas las combinaciones de enfermedades.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff0180",
   "metadata": {},
   "source": [
    "# Modelo de prueba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf17fa3",
   "metadata": {},
   "source": [
    "\n",
    "#### Descripción del modelo\n",
    "\n",
    "Hasta ahora habíamos trabajado con un enfoque **multiclase**, donde la variable objetivo `disease_class_final` codifica cada combinación posible de enfermedades. Para este tercer modelo decidimos replantear el problema de una forma más natural: como una **clasificación multi-etiqueta**.\n",
    "\n",
    "En este enfoque la red no intenta predecir una sola clase global, sino **seis etiquetas binarias a la vez**, una por cada enfermedad:\n",
    "\n",
    "- `Enfermedad cardiovascular_bin`  \n",
    "- `Diabetes_bin`  \n",
    "- `Asma_bin`  \n",
    "- `Cáncer_bin`  \n",
    "- `Obesidad_bin`  \n",
    "- `Depresión/Ansiedad_bin`\n",
    "\n",
    "La arquitectura que usamos es muy parecida a la de los modelos anteriores:\n",
    "\n",
    "- Capa de entrada con todas las variables explicativas (demográficas, estilo de vida, etc.) ya escaladas.  \n",
    "- Capa densa de **64 neuronas** con activación *ReLU*.  \n",
    "- Capa de **Dropout(0.3)** para evitar sobreajuste.  \n",
    "- Capa densa de **32 neuronas** con activación *ReLU*.  \n",
    "- Segunda capa de **Dropout(0.3)**.  \n",
    "- Capa de salida con **6 neuronas** y activación **sigmoid**, una probabilidad entre 0 y 1 para cada enfermedad.\n",
    "\n",
    "La red se entrena con:\n",
    "\n",
    "- **Función de pérdida:** `binary_crossentropy`, adecuada para problemas multi-etiqueta donde cada salida es un 0/1 independiente.  \n",
    "- **Optimizador:** `adam`.  \n",
    "- **Métrica de entrenamiento:** `accuracy` (se monitorea pero las conclusiones las sacamos sobre todo con F1 por etiqueta).\n",
    "\n",
    "Durante el entrenamiento (30 épocas) la *loss* de entrenamiento baja de forma moderada y la *loss* de validación se estabiliza alrededor de 0,47, con una accuracy de validación cercana al 18–19%. No se observa un sobreajuste extremo, pero tampoco una mejora espectacular, lo cual ya anticipa que el modelo tiene dificultades para separar bien los casos positivos de los negativos.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c28f0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: np.float64(2.3296447291788),\n",
       " 1: np.float64(0.15754544201343074),\n",
       " 2: np.float64(2.6586905948820205),\n",
       " 3: np.float64(0.8879023307436182),\n",
       " 4: np.float64(1.8674136321195145),\n",
       " 5: np.float64(0.18527095877721167),\n",
       " 6: np.float64(0.35813412122840005),\n",
       " 7: np.float64(2.5300442757748263),\n",
       " 8: np.float64(6.2745098039215685),\n",
       " 9: np.float64(11.477761836441895),\n",
       " 10: np.float64(0.6183813867202598),\n",
       " 11: np.float64(0.4516201874223778),\n",
       " 12: np.float64(19.607843137254903),\n",
       " 13: np.float64(1.5634160641000587),\n",
       " 14: np.float64(3.676470588235294),\n",
       " 15: np.float64(3.988035892323031),\n",
       " 16: np.float64(1.3181743285549514),\n",
       " 17: np.float64(0.30959752321981426),\n",
       " 18: np.float64(0.807183937039653),\n",
       " 19: np.float64(2.0460358056265986),\n",
       " 20: np.float64(1.1619462599854757),\n",
       " 21: np.float64(5.669737774627923),\n",
       " 22: np.float64(0.5697194131890044),\n",
       " 23: np.float64(1.3144922773578704),\n",
       " 24: np.float64(3.988035892323031),\n",
       " 25: np.float64(4.12796697626419),\n",
       " 26: np.float64(9.603841536614645),\n",
       " 27: np.float64(10.230179028132993),\n",
       " 28: np.float64(1.2515644555694618),\n",
       " 29: np.float64(8.714596949891067),\n",
       " 30: np.float64(5.228758169934641),\n",
       " 31: np.float64(18.823529411764707),\n",
       " 32: np.float64(13.071895424836601),\n",
       " 33: np.float64(9.049773755656108)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# calculo pesos por clase en base al y_train\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# lo convierto a diccionario {clase: peso}\n",
    "class_weights = {i: w for i, w in enumerate(class_weights)}\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "913f485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 - 0s - 2ms/step - accuracy: 0.0334 - loss: 2.7781 - val_accuracy: 0.0069 - val_loss: 3.4076\n",
      "Epoch 2/5\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.0340 - loss: 2.7703 - val_accuracy: 0.0069 - val_loss: 3.4237\n",
      "Epoch 3/5\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.0322 - loss: 2.8149 - val_accuracy: 0.0066 - val_loss: 3.4208\n",
      "Epoch 4/5\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.0315 - loss: 2.8093 - val_accuracy: 0.0072 - val_loss: 3.4331\n",
      "Epoch 5/5\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.0329 - loss: 2.7951 - val_accuracy: 0.0069 - val_loss: 3.4198\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_cat,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=2,\n",
    "    class_weight=class_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e32122d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo balanceado 1/10...\n",
      "\n",
      "Entrenando modelo balanceado 2/10...\n",
      "\n",
      "Entrenando modelo balanceado 3/10...\n",
      "\n",
      "Entrenando modelo balanceado 4/10...\n",
      "\n",
      "Entrenando modelo balanceado 5/10...\n",
      "\n",
      "Entrenando modelo balanceado 6/10...\n",
      "\n",
      "Entrenando modelo balanceado 7/10...\n",
      "\n",
      "Entrenando modelo balanceado 8/10...\n",
      "\n",
      "Entrenando modelo balanceado 9/10...\n",
      "\n",
      "Entrenando modelo balanceado 10/10...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n_models = 10\n",
    "epocas_por_modelo = 20\n",
    "n_por_clase = 200   # puedes jugar con este número\n",
    "\n",
    "modelos_bagging = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    print(f\"\\nEntrenando modelo balanceado {i+1}/{n_models}...\")\n",
    "\n",
    "    # construyo un bootstrap balanceado\n",
    "    dfs = []\n",
    "    for clase in np.unique(y_train):\n",
    "        idx_clase = np.where(y_train.values == clase)[0]\n",
    "        # si hay menos de n_por_clase, remuestro con reemplazo\n",
    "        muestras = np.random.choice(idx_clase, size=min(len(idx_clase), n_por_clase), replace=True)\n",
    "        dfs.append(pd.DataFrame({\n",
    "            \"y\": y_train.values[muestras],\n",
    "            \"idx\": muestras\n",
    "        }))\n",
    "    df_boot = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    idx_boot = df_boot[\"idx\"].values\n",
    "    y_boot = df_boot[\"y\"].values\n",
    "\n",
    "    X_boot = X_train_scaled[idx_boot]\n",
    "    y_boot_cat = to_categorical(y_boot, num_classes=num_classes)\n",
    "\n",
    "    m = crear_modelo_nn()\n",
    "    m.fit(\n",
    "        X_boot,\n",
    "        y_boot_cat,\n",
    "        epochs=epocas_por_modelo,\n",
    "        batch_size=128,\n",
    "        verbose=0\n",
    "    )\n",
    "    modelos_bagging.append(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa14bbf",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44293e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo balanceado 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo balanceado 2/5...\n",
      "\n",
      "Entrenando modelo balanceado 3/5...\n",
      "\n",
      "Entrenando modelo balanceado 4/5...\n",
      "\n",
      "Entrenando modelo balanceado 5/5...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n_models = 5\n",
    "epocas_por_modelo = 30\n",
    "n_por_clase = 100   \n",
    "\n",
    "modelos_bagging = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    print(f\"\\nEntrenando modelo balanceado {i+1}/{n_models}...\")\n",
    "\n",
    "    # construyo un bootstrap balanceado\n",
    "    dfs = []\n",
    "    for clase in np.unique(y_train):\n",
    "        idx_clase = np.where(y_train.values == clase)[0]\n",
    "        # si hay menos de n_por_clase, remuestro con reemplazo\n",
    "        muestras = np.random.choice(idx_clase, size=min(len(idx_clase), n_por_clase), replace=True)\n",
    "        dfs.append(pd.DataFrame({\n",
    "            \"y\": y_train.values[muestras],\n",
    "            \"idx\": muestras\n",
    "        }))\n",
    "    df_boot = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    idx_boot = df_boot[\"idx\"].values\n",
    "    y_boot = df_boot[\"y\"].values\n",
    "\n",
    "    X_boot = X_train_scaled[idx_boot]\n",
    "    y_boot_cat = to_categorical(y_boot, num_classes=num_classes)\n",
    "\n",
    "    m = crear_modelo_nn()\n",
    "    m.fit(\n",
    "        X_boot,\n",
    "        y_boot_cat,\n",
    "        epochs=epocas_por_modelo,\n",
    "        batch_size=128,\n",
    "        verbose=0\n",
    "    )\n",
    "    modelos_bagging.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e7ae8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols_multi = [\n",
    "    \"Enfermedad cardiovascular_bin\",\n",
    "    \"Diabetes_bin\",\n",
    "    \"Asma_bin\",\n",
    "    \"Cáncer_bin\",\n",
    "    \"Obesidad_bin\",\n",
    "    \"Depresión/Ansiedad_bin\"\n",
    "]\n",
    "\n",
    "X_multi = X.copy()           # las mismas features que ya preparaste\n",
    "y_multi = df[target_cols_multi].values  # matriz (n_samples, 6)\n",
    "\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_multi, y_multi,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler_m = StandardScaler()\n",
    "X_train_m_scaled = scaler_m.fit_transform(X_train_m)\n",
    "X_test_m_scaled  = scaler_m.transform(X_test_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04e88ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 1s - 13ms/step - accuracy: 0.1861 - loss: 0.5891 - val_accuracy: 0.1822 - val_loss: 0.4898\n",
      "Epoch 2/30\n",
      "100/100 - 0s - 2ms/step - accuracy: 0.2155 - loss: 0.5107 - val_accuracy: 0.1797 - val_loss: 0.4797\n",
      "Epoch 3/30\n",
      "100/100 - 0s - 2ms/step - accuracy: 0.2075 - loss: 0.4943 - val_accuracy: 0.1797 - val_loss: 0.4771\n",
      "Epoch 4/30\n",
      "100/100 - 0s - 2ms/step - accuracy: 0.1991 - loss: 0.4893 - val_accuracy: 0.1797 - val_loss: 0.4758\n",
      "Epoch 5/30\n",
      "100/100 - 0s - 2ms/step - accuracy: 0.1945 - loss: 0.4832 - val_accuracy: 0.1797 - val_loss: 0.4756\n",
      "Epoch 6/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1945 - loss: 0.4810 - val_accuracy: 0.1797 - val_loss: 0.4744\n",
      "Epoch 7/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1919 - loss: 0.4798 - val_accuracy: 0.1797 - val_loss: 0.4740\n",
      "Epoch 8/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1892 - loss: 0.4780 - val_accuracy: 0.1797 - val_loss: 0.4745\n",
      "Epoch 9/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1900 - loss: 0.4773 - val_accuracy: 0.1797 - val_loss: 0.4737\n",
      "Epoch 10/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1894 - loss: 0.4753 - val_accuracy: 0.1797 - val_loss: 0.4736\n",
      "Epoch 11/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1893 - loss: 0.4751 - val_accuracy: 0.1797 - val_loss: 0.4735\n",
      "Epoch 12/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1893 - loss: 0.4740 - val_accuracy: 0.1797 - val_loss: 0.4741\n",
      "Epoch 13/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1894 - loss: 0.4737 - val_accuracy: 0.1797 - val_loss: 0.4737\n",
      "Epoch 14/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1894 - loss: 0.4735 - val_accuracy: 0.1797 - val_loss: 0.4741\n",
      "Epoch 15/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1892 - loss: 0.4729 - val_accuracy: 0.1797 - val_loss: 0.4740\n",
      "Epoch 16/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1891 - loss: 0.4719 - val_accuracy: 0.1797 - val_loss: 0.4740\n",
      "Epoch 17/30\n",
      "100/100 - 0s - 2ms/step - accuracy: 0.1891 - loss: 0.4710 - val_accuracy: 0.1797 - val_loss: 0.4737\n",
      "Epoch 18/30\n",
      "100/100 - 0s - 2ms/step - accuracy: 0.1896 - loss: 0.4713 - val_accuracy: 0.1797 - val_loss: 0.4738\n",
      "Epoch 19/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1897 - loss: 0.4711 - val_accuracy: 0.1797 - val_loss: 0.4741\n",
      "Epoch 20/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1894 - loss: 0.4703 - val_accuracy: 0.1797 - val_loss: 0.4740\n",
      "Epoch 21/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1889 - loss: 0.4703 - val_accuracy: 0.1797 - val_loss: 0.4739\n",
      "Epoch 22/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1894 - loss: 0.4693 - val_accuracy: 0.1797 - val_loss: 0.4742\n",
      "Epoch 23/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1902 - loss: 0.4689 - val_accuracy: 0.1797 - val_loss: 0.4741\n",
      "Epoch 24/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1892 - loss: 0.4695 - val_accuracy: 0.1797 - val_loss: 0.4741\n",
      "Epoch 25/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1892 - loss: 0.4691 - val_accuracy: 0.1797 - val_loss: 0.4744\n",
      "Epoch 26/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1893 - loss: 0.4685 - val_accuracy: 0.1797 - val_loss: 0.4744\n",
      "Epoch 27/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1896 - loss: 0.4678 - val_accuracy: 0.1797 - val_loss: 0.4746\n",
      "Epoch 28/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1899 - loss: 0.4682 - val_accuracy: 0.1797 - val_loss: 0.4749\n",
      "Epoch 29/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1905 - loss: 0.4679 - val_accuracy: 0.1797 - val_loss: 0.4750\n",
      "Epoch 30/30\n",
      "100/100 - 0s - 1ms/step - accuracy: 0.1894 - loss: 0.4677 - val_accuracy: 0.1797 - val_loss: 0.4748\n"
     ]
    }
   ],
   "source": [
    "num_features = X_train_m_scaled.shape[1]\n",
    "num_outputs  = y_train_m.shape[1]\n",
    "\n",
    "model_multi = Sequential()\n",
    "model_multi.add(Dense(64, activation=\"relu\", input_shape=(num_features,)))\n",
    "model_multi.add(Dropout(0.3))\n",
    "model_multi.add(Dense(32, activation=\"relu\"))\n",
    "model_multi.add(Dropout(0.3))\n",
    "model_multi.add(Dense(num_outputs, activation=\"sigmoid\"))  # OJO: sigmoid aquí\n",
    "\n",
    "model_multi.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",   # OJO: binary aquí\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model_multi.fit(\n",
    "    X_train_m_scaled,\n",
    "    y_train_m,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13c808d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step\n",
      "Enfermedad cardiovascular_bin F1: 0.0\n",
      "Diabetes_bin F1: 0.0\n",
      "Asma_bin F1: 0.0\n",
      "Cáncer_bin F1: 0.0\n",
      "Obesidad_bin F1: 0.0\n",
      "Depresión/Ansiedad_bin F1: 0.6893236074270557\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_m = model_multi.predict(X_test_m_scaled)\n",
    "y_pred_m = (y_pred_proba_m >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for i, col in enumerate(target_cols_multi):\n",
    "    print(col, \"F1:\", f1_score(y_test_m[:, i], y_pred_m[:, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61192a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enfermedad cardiovascular_bin positivos: 0.3362\n",
      "Diabetes_bin positivos: 0.195\n",
      "Asma_bin positivos: 0.1169\n",
      "Cáncer_bin positivos: 0.0174\n",
      "Obesidad_bin positivos: 0.2537\n",
      "Depresión/Ansiedad_bin positivos: 0.5423\n"
     ]
    }
   ],
   "source": [
    "# proporción de 1s por enfermedad en el conjunto de entrenamiento\n",
    "for col in target_cols_multi:\n",
    "    print(col, \"positivos:\", df[col].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf007a",
   "metadata": {},
   "source": [
    "\n",
    "#### Resultados y conclusión\n",
    "\n",
    "Para evaluar el modelo usamos el conjunto de test y calculamos el **F1-score por enfermedad** (comparando la predicción binaria con las columnas `*_bin` reales). Los resultados son:\n",
    "\n",
    "- Enfermedad cardiovascular: **F1 ≈ 0,0**  \n",
    "- Diabetes: **F1 ≈ 0,0**  \n",
    "- Asma: **F1 ≈ 0,0**  \n",
    "- Cáncer: **F1 ≈ 0,0**  \n",
    "- Obesidad: **F1 ≈ 0,0**  \n",
    "- Depresión/Ansiedad: **F1 ≈ 0,69**\n",
    "\n",
    "Es decir, la red **solo consigue un rendimiento razonable en Depresión/Ansiedad**, mientras que prácticamente nunca acierta positivos en el resto de enfermedades (equivale a predecir casi todo como 0).\n",
    "\n",
    "Si miramos la proporción de positivos en el conjunto de datos se entiende por qué pasa esto:\n",
    "\n",
    "- Enfermedad cardiovascular: ~**33,6%** de positivos  \n",
    "- Diabetes: ~**19,5%** de positivos  \n",
    "- Asma: ~**11,7%** de positivos  \n",
    "- Cáncer: solo ~**1,7%** de positivos  \n",
    "- Obesidad: ~**25,4%** de positivos  \n",
    "- Depresión/Ansiedad: ~**54,2%** de positivos  \n",
    "\n",
    "Depresión/Ansiedad es claramente la condición más frecuente, mientras que Cáncer es muy rara y las demás se mueven en rangos intermedios. En este contexto, al modelo “le sale rentable” aprender bien la etiqueta más común y **ser muy conservador** con las demás: si casi siempre predice ausencia de enfermedad, la pérdida global no se dispara, pero el coste es que las etiquetas minoritarias quedan totalmente desatendidas (de ahí esos F1 ≈ 0).\n",
    "\n",
    "En resumen, este modelo multi-etiqueta nos deja varias ideas claras:\n",
    "\n",
    "- La red **sí es capaz de capturar cierta señal** para depresión/ansiedad (F1 ≈ 0,69), lo que sugiere que las variables disponibles contienen información útil para esa condición.  \n",
    "- Para las demás enfermedades, especialmente las menos frecuentes como Cáncer, el modelo **no alcanza a aprender patrones robustos** y termina prediciendo casi siempre 0.  \n",
    "- El problema de fondo vuelve a ser el mismo que veíamos en los modelos multiclase y en el bagging: el **fuerte desbalance de las etiquetas** y la falta de ejemplos suficientes para muchas enfermedades.\n",
    "\n",
    "Por tanto, el modelo multi-etiqueta aporta una visión más detallada (podemos ver enfermedad por enfermedad), pero confirma la misma conclusión general del proyecto: con el dataset tal como está, el rendimiento de las redes neuronales está limitado sobre todo por la distribución de los datos y no tanto por la arquitectura concreta del modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
